#!/usr/bin/env python3
"""
PrintShop çŸ¥è¯†åº“å‘é‡åµŒå…¥ç”Ÿæˆå™¨
ç”Ÿæˆ 34 ä¸ªçŸ¥è¯†åº“æ–‡æ¡£çš„å‘é‡åµŒå…¥ï¼Œæ”¯æŒ JSON è¾“å‡ºå’Œ PostgreSQL/pgvector å†™å…¥
"""

import os
import json
import hashlib
from pathlib import Path
from datetime import datetime

# å°è¯•å¯¼å…¥ä¾èµ–
try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    print("é”™è¯¯: éœ€è¦å®‰è£… sentence-transformers")
    print("è¿è¡Œ: pip install sentence-transformers")
    exit(1)

# é…ç½®
KNOWLEDGE_DIR = Path(__file__).parent.parent / "knowledge"
OUTPUT_JSON = Path(__file__).parent.parent / "embeddings" / "knowledge-vectors.json"
MODEL_NAME = "paraphrase-multilingual-MiniLM-L12-v2"  # æ”¯æŒä¸­æ–‡çš„å¤šè¯­è¨€æ¨¡å‹

# PostgreSQL é…ç½®ï¼ˆå¯é€‰ï¼‰
PG_CONFIG = {
    "host": "localhost",
    "port": 5432,
    "database": "printshop",
    "user": "postgres",
    "password": ""  # ä»ç¯å¢ƒå˜é‡è¯»å–
}


def load_markdown_files(knowledge_dir: Path) -> list[dict]:
    """åŠ è½½æ‰€æœ‰ markdown æ–‡ä»¶"""
    documents = []
    
    for md_file in knowledge_dir.rglob("*.md"):
        # è·³è¿‡ README æ–‡ä»¶
        if md_file.name == "README.md":
            continue
            
        relative_path = md_file.relative_to(knowledge_dir)
        category = relative_path.parts[0] if len(relative_path.parts) > 1 else "root"
        
        with open(md_file, "r", encoding="utf-8") as f:
            content = f.read()
        
        # æå–æ ‡é¢˜ï¼ˆç¬¬ä¸€ä¸ª # å¼€å¤´çš„è¡Œï¼‰
        title = md_file.stem
        for line in content.split("\n"):
            if line.startswith("# "):
                title = line[2:].strip()
                break
        
        # ç”Ÿæˆæ–‡æ¡£ IDï¼ˆåŸºäºè·¯å¾„çš„ hashï¼‰
        doc_id = hashlib.md5(str(relative_path).encode()).hexdigest()[:12]
        
        documents.append({
            "id": doc_id,
            "path": str(relative_path),
            "category": category,
            "title": title,
            "content": content,
            "char_count": len(content)
        })
    
    return documents


def chunk_document(doc: dict, max_chars: int = 1000, overlap: int = 200) -> list[dict]:
    """å°†é•¿æ–‡æ¡£åˆ†å—"""
    content = doc["content"]
    
    # çŸ­æ–‡æ¡£ä¸åˆ†å—
    if len(content) <= max_chars:
        return [doc]
    
    chunks = []
    start = 0
    chunk_idx = 0
    
    while start < len(content):
        end = start + max_chars
        
        # å°è¯•åœ¨æ®µè½è¾¹ç•Œåˆ‡åˆ†
        if end < len(content):
            # æ‰¾æœ€è¿‘çš„æ¢è¡Œç¬¦
            newline_pos = content.rfind("\n\n", start, end)
            if newline_pos > start + max_chars // 2:
                end = newline_pos
        
        chunk_content = content[start:end].strip()
        
        if chunk_content:
            chunks.append({
                "id": f"{doc['id']}_c{chunk_idx}",
                "path": doc["path"],
                "category": doc["category"],
                "title": f"{doc['title']} (Part {chunk_idx + 1})",
                "content": chunk_content,
                "char_count": len(chunk_content),
                "chunk_index": chunk_idx,
                "parent_id": doc["id"]
            })
            chunk_idx += 1
        
        start = end - overlap
    
    return chunks


def generate_embeddings(documents: list[dict], model: SentenceTransformer) -> list[dict]:
    """ç”Ÿæˆå‘é‡åµŒå…¥"""
    print(f"æ­£åœ¨ç”Ÿæˆ {len(documents)} ä¸ªæ–‡æ¡£çš„å‘é‡åµŒå…¥...")
    
    # æå–æ–‡æœ¬ï¼ˆæ ‡é¢˜ + å†…å®¹ï¼‰
    texts = [f"{doc['title']}\n\n{doc['content']}" for doc in documents]
    
    # æ‰¹é‡ç”ŸæˆåµŒå…¥
    embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)
    
    # æ·»åŠ åµŒå…¥åˆ°æ–‡æ¡£
    for doc, embedding in zip(documents, embeddings):
        doc["embedding"] = embedding.tolist()
        doc["embedding_dim"] = len(embedding)
    
    return documents


def save_to_json(documents: list[dict], output_path: Path):
    """ä¿å­˜åˆ° JSON æ–‡ä»¶"""
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºè¾“å‡ºç»“æ„
    output = {
        "metadata": {
            "model": MODEL_NAME,
            "generated_at": datetime.now().isoformat(),
            "total_documents": len(documents),
            "embedding_dim": documents[0]["embedding_dim"] if documents else 0
        },
        "documents": documents
    }
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å·²ä¿å­˜åˆ° {output_path}")
    print(f"   æ–‡ä»¶å¤§å°: {output_path.stat().st_size / 1024 / 1024:.2f} MB")


def save_to_postgres(documents: list[dict]):
    """ä¿å­˜åˆ° PostgreSQLï¼ˆéœ€è¦ pgvector æ‰©å±•ï¼‰"""
    try:
        import psycopg2
        from psycopg2.extras import execute_values
    except ImportError:
        print("è­¦å‘Š: æœªå®‰è£… psycopg2ï¼Œè·³è¿‡ PostgreSQL å†™å…¥")
        return False
    
    password = os.environ.get("PGPASSWORD", PG_CONFIG["password"])
    if not password:
        print("è­¦å‘Š: æœªè®¾ç½® PGPASSWORDï¼Œè·³è¿‡ PostgreSQL å†™å…¥")
        return False
    
    try:
        conn = psycopg2.connect(
            host=PG_CONFIG["host"],
            port=PG_CONFIG["port"],
            database=PG_CONFIG["database"],
            user=PG_CONFIG["user"],
            password=password
        )
        cur = conn.cursor()
        
        # ç¡®ä¿ pgvector æ‰©å±•å­˜åœ¨
        cur.execute("CREATE EXTENSION IF NOT EXISTS vector;")
        
        # åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        embedding_dim = documents[0]["embedding_dim"]
        cur.execute(f"""
            CREATE TABLE IF NOT EXISTS knowledge_embeddings (
                id VARCHAR(20) PRIMARY KEY,
                path VARCHAR(255),
                category VARCHAR(50),
                title VARCHAR(255),
                content TEXT,
                embedding vector({embedding_dim}),
                created_at TIMESTAMP DEFAULT NOW()
            );
        """)
        
        # æ¸…ç©ºæ—§æ•°æ®
        cur.execute("TRUNCATE knowledge_embeddings;")
        
        # æ‰¹é‡æ’å…¥
        values = [
            (
                doc["id"],
                doc["path"],
                doc["category"],
                doc["title"],
                doc["content"],
                doc["embedding"]
            )
            for doc in documents
        ]
        
        execute_values(
            cur,
            """
            INSERT INTO knowledge_embeddings (id, path, category, title, content, embedding)
            VALUES %s
            """,
            values,
            template="(%s, %s, %s, %s, %s, %s::vector)"
        )
        
        conn.commit()
        cur.close()
        conn.close()
        
        print(f"âœ… å·²å†™å…¥ PostgreSQL ({len(documents)} æ¡è®°å½•)")
        return True
        
    except Exception as e:
        print(f"âŒ PostgreSQL å†™å…¥å¤±è´¥: {e}")
        return False


def main():
    print("=" * 50)
    print("PrintShop çŸ¥è¯†åº“å‘é‡åµŒå…¥ç”Ÿæˆå™¨")
    print("=" * 50)
    
    # 1. åŠ è½½æ–‡æ¡£
    print(f"\nğŸ“‚ åŠ è½½çŸ¥è¯†åº“: {KNOWLEDGE_DIR}")
    documents = load_markdown_files(KNOWLEDGE_DIR)
    print(f"   æ‰¾åˆ° {len(documents)} ä¸ªæ–‡æ¡£")
    
    # 2. åˆ†å—å¤„ç†
    print("\nğŸ“„ æ–‡æ¡£åˆ†å—å¤„ç†...")
    chunked_docs = []
    for doc in documents:
        chunks = chunk_document(doc)
        chunked_docs.extend(chunks)
    print(f"   åˆ†å—åå…± {len(chunked_docs)} ä¸ªç‰‡æ®µ")
    
    # 3. åŠ è½½æ¨¡å‹
    print(f"\nğŸ¤– åŠ è½½æ¨¡å‹: {MODEL_NAME}")
    model = SentenceTransformer(MODEL_NAME)
    
    # 4. ç”ŸæˆåµŒå…¥
    print("\nâš¡ ç”Ÿæˆå‘é‡åµŒå…¥...")
    embedded_docs = generate_embeddings(chunked_docs, model)
    
    # 5. ä¿å­˜ç»“æœ
    print("\nğŸ’¾ ä¿å­˜ç»“æœ...")
    save_to_json(embedded_docs, OUTPUT_JSON)
    save_to_postgres(embedded_docs)
    
    # 6. ç»Ÿè®¡
    print("\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   åŸå§‹æ–‡æ¡£: {len(documents)}")
    print(f"   åˆ†å—ç‰‡æ®µ: {len(chunked_docs)}")
    print(f"   å‘é‡ç»´åº¦: {embedded_docs[0]['embedding_dim']}")
    
    categories = {}
    for doc in documents:
        cat = doc["category"]
        categories[cat] = categories.get(cat, 0) + 1
    
    print("   åˆ†ç±»ç»Ÿè®¡:")
    for cat, count in sorted(categories.items()):
        print(f"     - {cat}: {count}")
    
    print("\nâœ… å®Œæˆ!")


if __name__ == "__main__":
    main()
